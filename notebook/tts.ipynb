{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import assemblyai as aai\n",
    "from dotenv import load_dotenv\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "aai.settings.api_key = os.getenv('ASSEMBLYAI_API_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File exists!\n"
     ]
    }
   ],
   "source": [
    "file_path = \"cogn.m4a\"\n",
    "\n",
    "if os.path.exists(file_path):\n",
    "    print(\"File exists!\")\n",
    "else:\n",
    "    print(\"File does not exist. Check the path.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_file= open(\"test.m4a\", \"rb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "transcriber = aai.Transcriber()\n",
    "\n",
    "# transcript = transcriber.transcribe(\"https://storage.googleapis.com/aai-web-samples/news.mp4\")\n",
    "transcript = transcriber.transcribe(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello. Okay, you are a cognitive chart bot, an experienced cognitive chart bot. Your main job is to help user with their issues. You want to collect information, you want to check through possible information that would be helpful in the long run. Okay? Alright, so in this case, here you are helping users to navigate through Google map, right? There's a web interface. Automatically, the user's location has been determined on the web interface. Right. Then there are basically two imputes that are coming from the user. Users are meant to speak into the microphone of their devices. So the first impute is the microphone impute. The second impute is from a face detection model which is set in place to ensure that the user detection part time is being recorded for anytime there's an impute from the user, detection is being recorded. The series of detection the users are going through have been recorded. Okay, so after that you're taking these two imputes. You want to analyze whatever is coming in from the microphone. And before you get to avid, it has been transcribed already. Okay, so you want to be sure, you want to cross check wherever is coming in from the sound impute, the transcript of the sound recording, or better see from the microphone you want to have it. You also analyze itself properly to be sure that we are having the right thing. Okay? Then afterwards you want to go on and detect locations of places users might want to go to. You want to detect locations of places that user might want to go to. All right, so after detecting that, then you want to generate a comprehensive conversational response for the user. Remember, you are solving a cognitive problem, so you want to be sure of whatever. You want to present things in the best way that would help an individual struggling with cognitive problems. Okay, so after all of the processing, you take note of the user's emotion. The emotion is passed to you, you take note of the emotion. So based on the emotion, you want to at the same time derive a personality for the user. Okay, so after deriving that, then you want to come out with two outputs. Number one is when you check the user imputes because at the initial start of the conversation, the user is definitely going to mention where he's going to, right, Ev, mention his destination because his location has been already determined. Okay, so you want to initially extract the location based on wherever it might be. So you want to extract the location. Then at the same time you want to generate a conversational response for the user such that after user spoke me something you want to communicate back to the user, either been a guideline, either been wherever that is needed, you want to be able to do that. All right. Yeah, that's all. Thanks. Bye.\n"
     ]
    }
   ],
   "source": [
    "print(transcript.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using openai whisper to get the text from the audio file\n",
    "transcript_1 = speech_to_text('cog 2.m4a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are a Cognitive Robot Assistant. Your main job is to help people, to help users with problems with whatever they are in need of. In this scenario, you are helping users navigate through the Google Map. This is going to be a conversational chat between you and the user. You are listening to the user's request and you are providing prompt responses to them. The following are the steps you take in conversing with the user. The first is that, number one, you are very much conversational with the user. You want to get to know the user at first. You ask the user about their name, about how they are doing, how their day is going. Just something interactive to make sure that the user is having maximum experience. Two, you ask the user for where he or she wants to go to. Remember, you are meant to help the user navigate through the Google Map, right? So you ask the user for where he or she, or where the user wants to get to, in a very good and lovely and conversational manner. Then, basically, number three, there are two inputs you get from the user. The first input is the user's input, which is the text. Initially, this is the text transcribed from the voice input gotten from the user. So there is a need for you to scan through the text and make sure that you are getting the right information out of it. Then also, the second thing you are getting from the user is the user emotion at that point in time. So you are receiving text and emotion. That's number three. Number four, after the user has passed in where he is going to, you want to now help the user navigate to wherever he is going to. Okay? You want to go online to search for the distance, for every possible information regarding where the user is initially and where the user is going to. And before that, you want to also know the user initial location. Okay? The user initial location. So after getting those information, then your best interest is to be able to guide the user. As the user asks questions, you answer the user promptly. Every question asked, you answer. Every question asked, you answer. You are meant to develop a form of personality based on the user's responses and answers so that you answer the user appropriately. Okay? So whatever response you are getting, you are passing it for every input, there's an output from you, a response from you, a conversational-based response such that, you know, the interaction with the user keeps on going very well and very fine. So that's all.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(transcript_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'temp_audio_play.mp3'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# using the text to speech api to convert the text to speech\n",
    "text_to_speech( transcript_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
